{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38b6b9e4",
   "metadata": {},
   "source": [
    "# Arpitha Coorg\n",
    "#### Question 1: I used generative AI for getting started on  problem 5 to help debug my gradient descent implementation since I did not understand the error I was running into. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f0c330b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bd448fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "target = \"Sleep_Quality\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dac3214b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Columns: ['Gender', 'Occupation', 'BMI_Category', 'Blood_Pressure', 'Sleep_Disorder', 'ID']\n",
      "Train shape: (300, 7)\n",
      "Test shape: (74, 7)\n"
     ]
    }
   ],
   "source": [
    "#2a)\n",
    "\n",
    "non_numeric = (\n",
    "    train\n",
    "    .drop(columns=[target], errors = \"ignore\")\n",
    "    .select_dtypes(exclude=\"number\")\n",
    "    .columns\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "cols_drop = non_numeric.copy()\n",
    "\n",
    "if \"ID\" in train.columns:\n",
    "    cols_drop.append(\"ID\")\n",
    "\n",
    "train_clean = train.drop(columns=cols_drop)\n",
    "test_clean = test.drop(columns=cols_drop)\n",
    "\n",
    "print(\"Removed Columns:\" , cols_drop)\n",
    "print(\"Train shape:\", train_clean.shape)\n",
    "print(\"Test shape:\", test_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2208cbe",
   "metadata": {},
   "source": [
    "##### 2a: Removed Columns other than ID: ['Gender', 'Occupation', 'BMI_Category', 'Blood_Pressure', 'Sleep_Disorder']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "246f97ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 4.614076015572055\n",
      "Training MSE: 0.13100264319632646\n",
      "Training R^2: 0.9064110852787418\n",
      "          feature      coef  abs_coef\n",
      "1  Sleep_Duration  0.663838  0.663838\n",
      "3    Stress_Level -0.322086  0.322086\n",
      "4      Heart_Rate -0.021018  0.021018\n",
      "0             Age  0.013840  0.013840\n",
      "2  Activity_Level -0.000594  0.000594\n",
      "5     Daily_Steps  0.000092  0.000092\n"
     ]
    }
   ],
   "source": [
    "#2b)\n",
    "X_train = train_clean.drop(columns=[target])\n",
    "Y_train = train_clean[target]\n",
    "X_test = test_clean.drop(columns=[target])\n",
    "Y_test = test_clean[target]\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,Y_train)\n",
    "yhat_train = lr.predict(X_train)\n",
    "mse_train = mean_squared_error(Y_train,yhat_train)\n",
    "r2_train = r2_score(Y_train,yhat_train)\n",
    "\n",
    "coef_df = pd.DataFrame ({\n",
    "    \"feature\": X_train.columns,\n",
    "    \"coef\": lr.coef_\n",
    "})\n",
    "coef_df[\"abs_coef\"] = coef_df[\"coef\"].abs()\n",
    "coef_df = coef_df.sort_values(\"abs_coef\", ascending=False)\n",
    "\n",
    "\n",
    "print(\"Intercept:\", lr.intercept_)\n",
    "print(\"Training MSE:\", mse_train)\n",
    "print(\"Training R^2:\", r2_train)\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b4098600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MSE: 0.12919025461693848\n",
      "Testing R^2: 0.9127041171912198\n"
     ]
    }
   ],
   "source": [
    "#2c)\n",
    "yhat_test = lr.predict(X_test)\n",
    "mse_test = mean_squared_error(Y_test,yhat_test)\n",
    "r2_test = r2_score(Y_test,yhat_test)\n",
    "print(\"Testing MSE:\", mse_test)\n",
    "print(\"Testing R^2:\", r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "288c2399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.08934405084462635\n",
      "Training R2: 0.9361721828557429\n",
      "Testing MSE: 0.08169486165862917\n",
      "Testing R2: 0.9447974996985867\n"
     ]
    }
   ],
   "source": [
    "#2d)\n",
    "cols = [\"BMI_Category\", \"Sleep_Disorder\"]\n",
    "\n",
    "def keep_numeric (df, target, cols):\n",
    "    df2 = df.copy()\n",
    "    if \"ID\" in df2.columns:\n",
    "        df2 = df2.drop(columns=[\"ID\"])\n",
    "    numeric_cols = (\n",
    "        df2.drop(columns=[target], errors = \"ignore\")\n",
    "        .select_dtypes(include = \"number\")\n",
    "        .columns\n",
    "        .tolist()\n",
    "    )\n",
    "    keep_cols = [target] + numeric_cols + cols\n",
    "    return df2[keep_cols]\n",
    "train_second = keep_numeric(train, target, cols)\n",
    "test_second = keep_numeric(test, target, cols)\n",
    "enc_train_second = pd.get_dummies(train_second, columns=cols, drop_first=True)\n",
    "enc_test_second = pd.get_dummies(test_second, columns=cols, drop_first=True)\n",
    "enc_train_second, enc_test_second = enc_train_second.align(\n",
    "    enc_test_second, join = \"left\", axis = 1, fill_value= 0\n",
    ")\n",
    "\n",
    "X_train2 = enc_train_second.drop(columns = [target])\n",
    "Y_train2 = enc_train_second[target]\n",
    "X_test2 = enc_test_second.drop(columns=[target])\n",
    "Y_test2 = enc_test_second[target]\n",
    "\n",
    "lr2 = LinearRegression()\n",
    "lr2.fit(X_train2, Y_train2)\n",
    "yhat_train2 = lr2.predict(X_train2)\n",
    "yhat_test2 = lr2.predict(X_test2)\n",
    "new_train_mse = mean_squared_error(Y_train2, yhat_train2)\n",
    "new_train_r2 = r2_score(Y_train2, yhat_train2)\n",
    "new_test_mse = mean_squared_error(Y_test2, yhat_test2)\n",
    "new_test_r2 = r2_score(Y_test2, yhat_test2)\n",
    "\n",
    "print(\"Training MSE:\", new_train_mse)\n",
    "print(\"Training R2:\", new_train_r2)\n",
    "print(\"Testing MSE:\", new_test_mse)\n",
    "print(\"Testing R2:\", new_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3b2836",
   "metadata": {},
   "source": [
    "##### 2d: I chose BMI_Category and Sleep_Disorder because they have relevant health related information. These features were encoded using one-hot encoding to avoid having any artificial numerical ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e4c43d",
   "metadata": {},
   "source": [
    "##### 2e: The features that have the largest absolute coefficient values contribute most to the linear regression model, showing that they have the strongest influence on sleep quality. The model has a low MSE value, which means the predictions are more accurate. Since the training and testing MSE values are similar, the model generalizes well and does not have significant overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bdbbcfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed-form intercept: 4.614076015227511\n",
      "Closed-form coefficients: [ 1.38404261e-02  6.63838357e-01 -5.94043432e-04 -3.22086472e-01\n",
      " -2.10175847e-02  9.15066625e-05]\n"
     ]
    }
   ],
   "source": [
    "#3a)\n",
    "X_train = train_clean.drop(columns = [target]).to_numpy()\n",
    "Y_train = train_clean[target].to_numpy()\n",
    "X_test = test_clean.drop(columns = [target]).to_numpy()\n",
    "Y_test = test_clean[target].to_numpy()\n",
    "\n",
    "def fit_closed_form(X,Y):\n",
    "    Xb = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    theta = np.linalg.pinv(Xb.T @ Xb) @ (Xb.T @ Y)\n",
    "    return theta\n",
    "\n",
    "def predict_closed_form(X,theta):\n",
    "    Xb = np.c_[np.ones((X.shape[0], 1)),X]\n",
    "    return Xb @ theta\n",
    "\n",
    "theta = fit_closed_form(X_train, Y_train)\n",
    "intercept_cf = theta[0]\n",
    "coefs_cf = theta[1:]\n",
    "\n",
    "print(\"Closed-form intercept:\", intercept_cf)\n",
    "print(\"Closed-form coefficients:\", coefs_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a2d8f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed Form Train MSE: 0.13100264319632643\n",
      "Closed Form Train R2 : 0.9064110852787419\n",
      "Closed Form Test MSE: 0.12919025461879174\n",
      "Closed Form Test R2 : 0.9127041171899675\n",
      "sklearn Train MSE: 0.13100264319632646\n",
      "sklearn Train R2 : 0.9064110852787418\n",
      "sklearn Test MSE: 0.12919025461693848\n",
      "sklearn Test R2 : 0.9127041171912198\n"
     ]
    }
   ],
   "source": [
    "#3b)\n",
    "yhat_train_cf = predict_closed_form(X_train, theta)\n",
    "yhat_test_cf = predict_closed_form(X_test, theta)\n",
    "mse_train_cf = mean_squared_error(Y_train, yhat_train_cf)\n",
    "r2_train_cf = r2_score(Y_train, yhat_train_cf)\n",
    "mse_test_cf = mean_squared_error(Y_test, yhat_test_cf)\n",
    "r2_test_cf = r2_score(Y_test, yhat_test_cf)\n",
    "\n",
    "\n",
    "print(\"Closed Form Train MSE:\", mse_train_cf)\n",
    "print(\"Closed Form Train R2 :\", r2_train_cf)\n",
    "print(\"Closed Form Test MSE:\", mse_test_cf)\n",
    "print(\"Closed Form Test R2 :\", r2_test_cf)\n",
    "\n",
    "X_train_df = train_clean.drop(columns=[target])\n",
    "X_test_df = test_clean.drop(columns=[target])\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_df, train_clean[target])\n",
    "yhat_train_sk = lr.predict(X_train_df)\n",
    "yhat_test_sk = lr.predict(X_test_df)\n",
    "mse_train_sk = mean_squared_error(train_clean[target], yhat_train_sk)\n",
    "r2_train_sk = r2_score(train_clean[target], yhat_train_sk)\n",
    "mse_test_sk = mean_squared_error(test_clean[target], yhat_test_sk)\n",
    "r2_test_sk = r2_score(test_clean[target], yhat_test_sk)\n",
    "\n",
    "\n",
    "print(\"sklearn Train MSE:\", mse_train_sk)\n",
    "print(\"sklearn Train R2 :\", r2_train_sk)\n",
    "print(\"sklearn Test MSE:\", mse_test_sk)\n",
    "print(\"sklearn Test R2 :\", r2_test_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32bd8e6",
   "metadata": {},
   "source": [
    "##### 3b: The results are almost identical, which makes sense since both methods solve the same least squares optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f13409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4a)\n",
    "def poly_features(X,p):\n",
    "    return np.column_stack([X**i for i in range(1, p+1)])\n",
    "\n",
    "X_train = train_clean[\"Sleep_Duration\"].to_numpy()\n",
    "Y_train = train_clean[target].to_numpy()\n",
    "X_test = test_clean[\"Sleep_Duration\"].to_numpy()\n",
    "Y_test = test_clean[target].to_numpy()\n",
    "P = 2\n",
    "X_train_poly = poly_features(X_train, P)\n",
    "X_test_poly = poly_features(X_test, P)\n",
    "\n",
    "theta_p = fit_closed_form(X_train_poly, Y_train)\n",
    "\n",
    "yhat_train = predict_closed_form(X_train_poly, theta_p)\n",
    "yhat_test = predict_closed_form(X_test_poly, theta_p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3e981",
   "metadata": {},
   "source": [
    "##### 4a: I used polynomial regression by expanding the feature X into [X,X^2,...X^P] and fitting the model using the closed form least squares solutions. I made predictions by applying the learned parameters to the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7507faf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polynomial Degree(p)</th>\n",
       "      <th>Train MSE</th>\n",
       "      <th>Train R2</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Test R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.304066</td>\n",
       "      <td>0.782774</td>\n",
       "      <td>0.357803</td>\n",
       "      <td>0.758227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.299796</td>\n",
       "      <td>0.785824</td>\n",
       "      <td>0.351232</td>\n",
       "      <td>0.762667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.292250</td>\n",
       "      <td>0.791215</td>\n",
       "      <td>0.331388</td>\n",
       "      <td>0.776076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.290183</td>\n",
       "      <td>0.792692</td>\n",
       "      <td>0.327831</td>\n",
       "      <td>0.778479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Polynomial Degree(p)  Train MSE  Train R2  Test MSE   Test R2\n",
       "0                     1   0.304066  0.782774  0.357803  0.758227\n",
       "1                     2   0.299796  0.785824  0.351232  0.762667\n",
       "2                     3   0.292250  0.791215  0.331388  0.776076\n",
       "3                     5   0.290183  0.792692  0.327831  0.778479"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4b)\n",
    "results = []\n",
    "for p in [1,2,3,5]:\n",
    "    X_train_poly = poly_features(X_train, p)\n",
    "    X_test_poly = poly_features(X_test, p)\n",
    "    theta_p = fit_closed_form(X_train_poly, Y_train)\n",
    "    yhat_train = predict_closed_form(X_train_poly, theta_p)\n",
    "    yhat_test = predict_closed_form(X_test_poly, theta_p)\n",
    "\n",
    "    results.append({\n",
    "        \"Polynomial Degree(p)\": p,\n",
    "        \"Train MSE\": mean_squared_error(Y_train, yhat_train),\n",
    "        \"Train R2\": r2_score(Y_train, yhat_train),\n",
    "        \"Test MSE\": mean_squared_error(Y_test, yhat_test),\n",
    "        \"Test R2\": r2_score(Y_test, yhat_test)\n",
    "    })\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a94453c",
   "metadata": {},
   "source": [
    "##### 4b: As the degree inceases, the training MSE decreases and the training R^2 increases, showing a better fit to the training data. Increase the degree to 5 improves the models ability to capture the relationship between sleep duration and sleep quality without a lot of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe584eb",
   "metadata": {},
   "source": [
    "##### 4c: As p increases, the training MSE decreases because the model gains more parameters and can fit the training data more closely. However, the testing MSE can eventually increase for larger p values because overly complex models may overfit the training data, and reducing generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8fbcbd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta without normalization:\n",
      "[-4.92570559e+192 -2.09873388e+194 -3.51803253e+193 -3.14580975e+194\n",
      " -2.66199337e+193 -3.45255861e+194 -3.57417529e+196]\n",
      "\n",
      "Theta with normalization:\n",
      "[ 1.47385408e-03  1.08091190e-04  2.09301877e-04  3.87599077e-05\n",
      " -2.11778280e-04 -1.51590152e-04 -1.09932380e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2t/h_cypqr94zx0kjmx1phwr9h40000gn/T/ipykernel_48895/943915138.py:30: RuntimeWarning: overflow encountered in square\n",
      "  mse = np.mean(err**2)\n"
     ]
    }
   ],
   "source": [
    "#5a)\n",
    "X_train_df = train_clean.drop(columns=[target])\n",
    "Y_train = train_clean[target].to_numpy()\n",
    "X_test_df = test_clean.drop(columns=[target])\n",
    "Y_test = test_clean[target].to_numpy()\n",
    "X_train_raw = X_train_df.to_numpy(dtype=float)\n",
    "X_test_raw = X_test_df.to_numpy(dtype=float)\n",
    "\n",
    "def add_bias(X):\n",
    "    return np.c_[np.ones((X.shape[0],1)),X]\n",
    "\n",
    "def normalize_train_test(X_train, X_test):\n",
    "    mu = X_train.mean(axis=0)\n",
    "    sigma = X_train.std(axis=0)\n",
    "    sigma[sigma == 0] = 1.0\n",
    "    X_train_n = (X_train - mu)/sigma\n",
    "    X_test_n = (X_test - mu)/sigma\n",
    "    return X_train_n, X_test_n, mu, sigma\n",
    "\n",
    "def gd_linear_regression(X,Y, alpha=0.1, iters=100):\n",
    "    n, d = X.shape\n",
    "    w = np.zeros(d)\n",
    "    history = []\n",
    "\n",
    "    for t in range(iters):\n",
    "        yhat = X @ w\n",
    "        err = yhat - Y\n",
    "        grad = (2.0/n) * (X.T @ err)\n",
    "        w = w - alpha * grad\n",
    "        mse = np.mean(err**2)\n",
    "        history.append(mse)\n",
    "    return w, history\n",
    "\n",
    "def eval_model(Xb, Y, w):\n",
    "    yhat = Xb @ w\n",
    "    return mean_squared_error(Y, yhat), r2_score(Y, yhat)\n",
    "\n",
    "Xb_train_raw = add_bias(X_train_raw)\n",
    "Xb_test_raw = add_bias(X_test_raw)\n",
    "\n",
    "alpha_try = 0.1\n",
    "w_raw, hist_raw = gd_linear_regression(Xb_train_raw, Y_train, alpha=1e-6, iters= 100)\n",
    "\n",
    "\n",
    "\n",
    "X_train_n, X_test_n, mu, sigma = normalize_train_test(X_train_raw, X_test_raw)\n",
    "Xb_train_n = add_bias(X_train_n)\n",
    "Xb_test_n = add_bias(X_test_n)\n",
    "\n",
    "w_norm, hist_norm = gd_linear_regression(Xb_train_n, Y_train, alpha=1e-6, iters = 100)\n",
    "\n",
    "train_mse_norm, train_r2_norm = eval_model(Xb_train_n, Y_train, w_norm)\n",
    "test_mse_norm, test_r2_norm = eval_model(Xb_test_n, Y_test, w_norm)\n",
    "\n",
    "print(\"Theta without normalization:\")\n",
    "print(w_raw)\n",
    "print(\"\\nTheta with normalization:\")\n",
    "print(w_norm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ead3ae",
   "metadata": {},
   "source": [
    "##### 5a: I implemented gradient descent for linear regression. by adding a bias term, initializing weights to zero,a nd updating the formula to decrease the MSE. Without normalization graident decent can become unstable because the features have different scales. After normalizing each feture using the training mean and standard devation, training becomes stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f906463e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>iterations</th>\n",
       "      <th>Train MSE</th>\n",
       "      <th>Train R2</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Test R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>3.686246e+01</td>\n",
       "      <td>-2.533472e+01</td>\n",
       "      <td>3.477731e+01</td>\n",
       "      <td>-2.249957e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>50</td>\n",
       "      <td>7.357906e+00</td>\n",
       "      <td>-4.256523e+00</td>\n",
       "      <td>7.201585e+00</td>\n",
       "      <td>-3.866224e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>1.095132e+00</td>\n",
       "      <td>2.176324e-01</td>\n",
       "      <td>1.072739e+00</td>\n",
       "      <td>2.751334e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>10</td>\n",
       "      <td>7.656346e-01</td>\n",
       "      <td>4.530270e-01</td>\n",
       "      <td>7.475378e-01</td>\n",
       "      <td>4.948770e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.10</td>\n",
       "      <td>50</td>\n",
       "      <td>1.325647e-01</td>\n",
       "      <td>9.052952e-01</td>\n",
       "      <td>1.276342e-01</td>\n",
       "      <td>9.137556e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.10</td>\n",
       "      <td>100</td>\n",
       "      <td>1.312959e-01</td>\n",
       "      <td>9.062016e-01</td>\n",
       "      <td>1.282910e-01</td>\n",
       "      <td>9.133117e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.50</td>\n",
       "      <td>10</td>\n",
       "      <td>8.683347e+03</td>\n",
       "      <td>-6.202425e+03</td>\n",
       "      <td>9.328417e+03</td>\n",
       "      <td>-6.302358e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.50</td>\n",
       "      <td>50</td>\n",
       "      <td>2.660853e+19</td>\n",
       "      <td>-1.900926e+19</td>\n",
       "      <td>2.858224e+19</td>\n",
       "      <td>-1.931347e+19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.50</td>\n",
       "      <td>100</td>\n",
       "      <td>6.066612e+38</td>\n",
       "      <td>-4.334017e+38</td>\n",
       "      <td>6.516609e+38</td>\n",
       "      <td>-4.403375e+38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha  iterations     Train MSE      Train R2      Test MSE       Test R2\n",
       "0   0.01          10  3.686246e+01 -2.533472e+01  3.477731e+01 -2.249957e+01\n",
       "1   0.01          50  7.357906e+00 -4.256523e+00  7.201585e+00 -3.866224e+00\n",
       "2   0.01         100  1.095132e+00  2.176324e-01  1.072739e+00  2.751334e-01\n",
       "3   0.10          10  7.656346e-01  4.530270e-01  7.475378e-01  4.948770e-01\n",
       "4   0.10          50  1.325647e-01  9.052952e-01  1.276342e-01  9.137556e-01\n",
       "5   0.10         100  1.312959e-01  9.062016e-01  1.282910e-01  9.133117e-01\n",
       "6   0.50          10  8.683347e+03 -6.202425e+03  9.328417e+03 -6.302358e+03\n",
       "7   0.50          50  2.660853e+19 -1.900926e+19  2.858224e+19 -1.931347e+19\n",
       "8   0.50         100  6.066612e+38 -4.334017e+38  6.516609e+38 -4.403375e+38"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5b)\n",
    "alphas = [0.01, 0.1, 0.5]\n",
    "iters_list = [10, 50, 100]\n",
    "\n",
    "rows = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    for iters in iters_list:\n",
    "        w, _ = gd_linear_regression(\n",
    "            Xb_train_n, Y_train, alpha=alpha, iters=iters\n",
    "        )\n",
    "        train_mse, train_r2 = eval_model(Xb_train_n, Y_train, w)\n",
    "        test_mse, test_r2 = eval_model(Xb_test_n, Y_test, w)\n",
    "        rows.append({\n",
    "            \"alpha\": alpha,\n",
    "            \"iterations\": iters,\n",
    "            \"Train MSE\": train_mse,\n",
    "            \"Train R2\": train_r2,\n",
    "            \"Test MSE\": test_mse,\n",
    "            \"Test R2\": test_r2\n",
    "        })\n",
    "results = pd.DataFrame(rows)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1752415",
   "metadata": {},
   "source": [
    "##### 5c: With a small learning rate, the gradient descent converges slowly. Early iterations have high MSE and negative R^2, and reasonable performance is reached only have a lot of interations. A moderate learning rate converges quickly and stably, which means the MSE is low and the R^2 is high by about 50 iterations. A large learning rate causes divergence, with a large MSE and a negative R^2. Overall normalization and an appropriate learning rate allow gradient descent to converge to the optimal solution efficiently. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
